---
title: "Homework 5"
author: "Yanru Fang"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Homework 5

For this assignment, we will be working with the file `"pokemon.csv"`, found in `/data`. The file is from Kaggle: <https://www.kaggle.com/abcsds/pokemon>.

The [Pokémon](https://www.pokemon.com/us/) franchise encompasses video games, TV shows, movies, books, and a card game. This data set was drawn from the video game series and contains statistics about 721 Pokémon, or "pocket monsters." In Pokémon games, the user plays as a trainer who collects, trades, and battles Pokémon to (a) collect all the Pokémon and (b) become the champion Pokémon trainer.

Each Pokémon has a [primary type](https://bulbapedia.bulbagarden.net/wiki/Type) (some even have secondary types). Based on their type, a Pokémon is strong against some types, and vulnerable to others. (Think rock, paper, scissors.) A Fire-type Pokémon, for example, is vulnerable to Water-type Pokémon, but strong against Grass-type.

![Fig 1. Vulpix, a Fire-type fox Pokémon from Generation 1 (also my favorite Pokémon!) ](images/vulpix.png){width="196"}

The goal of this assignment is to build a statistical learning model that can predict the **primary type** of a Pokémon based on its generation, legendary status, and six battle statistics. *This is an example of a **classification problem**, but these models can also be used for **regression problems***.

Read in the file and familiarize yourself with the variables using `pokemon_codebook.txt`.

```{r}
library(tidymodels)
library(ggplot2)
library(corrr)
library(discrim)
library(ranger)
library(glmnet)
library(corrplot)
library(vip)
library(forcats)
tidymodels_prefer()

data <- read.csv('/Users/fangyaner/Desktop/homework-5/data/Pokemon.csv')
head(data)
```

### Exercise 1

Install and load the `janitor` package. Use its `clean_names()` function on the Pokémon data, and save the results to work with for the rest of the assignment. What happened to the data? Why do you think `clean_names()` is useful?
```{r}
library(janitor)
pokemon <- clean_names(data)
head(pokemon)
```
* The variables name change to the lower cases and replace dots by underscores. The reason that `clean_names()` is useful because it can make data variables notations consistent and easy to read and manipulates. When there is many columns with different naming styles, `clean_names()` is very helpful.\

### Exercise 2

Using the entire data set, create a bar chart of the outcome variable, `type_1`.
```{r}
pokemon %>% 
  ggplot(aes(type_1)) +
  geom_bar()
```

How many classes of the outcome are there? Are there any Pokémon types with very few Pokémon? If so, which ones?

* From the bar chart above, there are 18 types of the outcome. The Pokemon type Flying and Fairy has very few Pokemon.\

For this assignment, we'll handle the rarer classes by grouping them, or "lumping them," together into an 'other' category. [Using the `forcats` package](https://forcats.tidyverse.org/), determine how to do this, and **lump all the other levels together except for the top 6 most frequent** (which are Bug, Fire, Grass, Normal, Water, and Psychic).
```{r}
pokemon$type_1 <- fct_lump_n(pokemon$type_1, n=6, other_level = 'other')
```

Convert `type_1` and `legendary` to factors.
```{r}
pokemon <- pokemon %>%
  mutate(type_1 = factor(type_1), legendary = factor(legendary))
head(pokemon)
```

### Exercise 3

Perform an initial split of the data. Stratify by the outcome variable. You can choose a proportion to use. Verify that your training and test sets have the desired number of observations.

Next, use *v*-fold cross-validation on the training set. Use 5 folds. Stratify the folds by `type_1` as well. *Hint: Look for a `strata` argument.*
```{r}
set.seed(123)
pokemon_split <- initial_split(pokemon, prop = 0.7, strata = type_1)
pokemon_train <- training(pokemon_split)
pokemon_test <- testing(pokemon_split)
dim(pokemon_train)
dim(pokemon_test)

set.seed(123)
pokemon_folds <- vfold_cv(data = pokemon_train, v = 5, strata = type_1)
```
* The training and test sets have the desired number of observations. There are 558 observations in training set, which is 70% of the entire data. There are 242 observations in testing set. \

Why do you think doing stratified sampling for cross-validation is useful?

* The reason for doing stratified sampling for cross-validation is useful because it can ensure the distribution of the variables remain the same. It also can make model more accuracy.\

### Exercise 4

Create a correlation matrix of the training set, using the `corrplot` package. *Note: You can choose how to handle the categorical variables for this plot; justify your decision(s).*
```{r}
matrix <- subset(pokemon_train, 
                 select = c(hp, attack, defense, sp_atk, sp_def, speed)) %>%
  cor(use = 'complete.obs')
corrplot(cor(matrix), type = 'lower', method = 'number', diag = FALSE)
```

What relationships, if any, do you notice?

* From the correlation matrix above, we can see that the `defense` and `speed`, `sp_def` and `attack` are negatively correlated with each other. Also, `speed` and `sp_atk` are positively correlated with each other. The categorical variables are not include in this correlation matrix, since they are usually not related to numerical variables.\
 
### Exercise 5

Set up a recipe to predict `type_1` with `legendary`, `generation`, `sp_atk`, `attack`, `speed`, `defense`, `hp`, and `sp_def`.

-   Dummy-code `legendary` and `generation`;

-   Center and scale all predictors.
```{r}
pokemon_recipe <- recipe(type_1 ~ legendary + generation + sp_atk + attack + 
                           speed + defense + hp + sp_def, 
                         data = pokemon_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_predictors())
```

### Exercise 6

We'll be fitting and tuning an elastic net, tuning `penalty` and `mixture` (use `multinom_reg()` with the `glmnet` engine).

Set up this model and workflow. Create a regular grid for `penalty` and `mixture` with 10 levels each; `mixture` should range from 0 to 1. For this assignment, let `penalty` range from 0.01 to 3 (this is on the `identity_trans()` scale; note that you'll need to specify these values in base 10 otherwise).
```{r}
ela_mod <- multinom_reg(penalty = tune(), mixture = tune()) %>%
  set_mode('classification') %>%
  set_engine('glmnet')

ela_wkflow <- workflow() %>%
  add_recipe(pokemon_recipe) %>%
  add_model(ela_mod)

ela_grid <- grid_regular(penalty(range = c(0.01, 3),
                                 trans = identity_trans()),
                         mixture(range = c(0, 1)),
                         levels = 10)
```

### Exercise 7

Now set up a random forest model and workflow. Use the `ranger` engine and set `importance = "impurity"`; we'll be tuning `mtry`, `trees`, and `min_n`. Using the documentation for `rand_forest()`, explain in your own words what each of these hyperparameters represent.

Create a regular grid with 8 levels each. You can choose plausible ranges for each hyperparameter. Note that `mtry` should not be smaller than 1 or larger than 8. **Explain why neither of those values would make sense.**
```{r}
ran_mod <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_mode('classification') %>%
  set_engine('ranger', importance = 'impurity')

ran_wkflow <- workflow() %>%
  add_recipe(pokemon_recipe) %>%
  add_model(ran_mod)

ran_grid <- grid_regular(mtry(range = c(1,8)),
                         trees(range = c(200, 600)),
                         min_n(range = c(10, 20)),
                         levels = 8)
```

* `mtry` specifies the number of variables randomly sampled as candidates at each split when building the trees. `trees` represents the number of trees in the forest. `min_n` is the minimum number of data points in a node required to attempt a split.\

* Since we have 8 variables in total, and we at least need one variable to decide on a split at each node. Thus, `mtry` should not be smaller than 1 or larger than 8.\

What type of model does `mtry = 8` represent?

* `mtry` represents the number of predictors in random tree process. There are 8 predictors. All the predictors are being used for the random tree process. There is a random Forest Model with 8 predictors.\

### Exercise 8

Fit all models to your folded data using `tune_grid()`.

**Note: Tuning your random forest model will take a few minutes to run, anywhere from 5 minutes to 15 minutes and up. Consider running your models outside of the .Rmd, storing the results, and loading them in your .Rmd to minimize time to knit. We'll go over how to do this in lecture.**

Use `autoplot()` on the results. What do you notice? Do larger or smaller values of `penalty` and `mixture` produce better ROC AUC? What about values of `min_n`, `trees`, and `mtry`?
```{r}
ela_fit <- tune_grid(object = ela_wkflow,
                     resamples = pokemon_folds,
                     grid = ela_grid)
save(ela_fit, file = 'ela_fit.rda')
load('ela_fit.rda')
autoplot(ela_fit) + theme_minimal()
```

```{r}
ran_fit <- tune_grid(object = ran_wkflow,
                     resamples = pokemon_folds,
                     grid = ran_grid)
save(ran_fit, file = 'ran_fit.rda')
load('ran_fit.rda')
autoplot(ran_fit) + theme_minimal()
```

* From the plot above, for the elastic net model, smaller values of `penalty` and `mixture` tend produce better ROC AUC. For the random forest model, it seems there is no clear pattern across to different value of `mtry`, `min_n`, and `trees`.\

What elastic net model and what random forest model perform the best on your folded data? (What specific values of the hyperparameters resulted in the optimal ROC AUC?)
```{r}
best_ela <- select_by_one_std_err(ela_fit, metric = 'roc_auc', penalty, mixture)
best_ela

best_ran <- select_best(ran_fit, metric = 'roc_auc', mtry, trees, min_n)
best_ran
```
* The elastic net model perform the best on folded data with parameters mixture = 0, penalty = 0.01.\
* The random forest model perform the best on folded data with parameter mtry = 3, trees = 314, min_n = 20.\

### Exercise 9

Select your optimal [**random forest model**]{.underline}in terms of `roc_auc`. Then fit that model to your training set and evaluate its performance on the testing set.

Using the **training** set:

-   Create a variable importance plot, using `vip()`. *Note that you'll still need to have set `importance = "impurity"` when fitting the model to your entire training set in order for this to work.*

    -   What variables were most useful? Which were least useful? Are these results what you expected, or not?
```{r}
ran_final <- finalize_workflow(ran_wkflow, best_ran)
ran_final_fit <- fit(ran_final, data = pokemon_train)
final_predicted <- augment(ran_final_fit, new_data = pokemon_test) %>%
  select(type_1, starts_with('.pred'))
final_predicted %>% roc_auc(type_1, .pred_Bug:.pred_other)
ran_final_fit %>% 
  extract_fit_parsnip() %>%
  vip() + theme_minimal()
```

* From the plot above, we can see variable `sp_atk` is the most useful. The variable `legendary_True` is the least useful.\

Using the testing set:

-   Create plots of the different ROC curves, one per level of the outcome variable;
```{r}
final_predicted %>%
  roc_curve(type_1, .pred_Bug:.pred_other) %>%
  autoplot()
```

-   Make a heat map of the confusion matrix.
```{r}
conf_mat(final_predicted, truth = type_1, estimate = .pred_class) %>%
  autoplot(type = 'heatmap')
```

### Exercise 10

How did your best random forest model do on the testing set?

Which Pokemon types is the model best at predicting, and which is it worst at? (Do you have any ideas why this might be?)

* Based on the roc_auc, the best random model did not perform well on the testing set. The "Normal" and 'Psychic' type are the model best at predicting, and the "Water", 'other', and 'Grass' are the worst at predicting.\
